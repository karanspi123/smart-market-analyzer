Below is an updated version of the "Smart Market Analyzer" prompt, incorporating a Transformer with self-attention for price prediction and a CNN for pattern recognition, while adding specific steps to mitigate the failures you outlined. These mitigations are woven into the architecture, code design, and validation processes to ensure robustness, reliability, and adaptability. The prompt remains actionable for creating an end-to-end working codebase with Claude 3.5 Sonnet.
End-to-End Prompt: Smart Market Analyzer with Transformer, CNN, and Failure Mitigations
Objective
Develop a modular, microservices-based system to:
Predict stock prices using a Transformer with self-attention, leveraging historical data and Shannon entropy for uncertainty quantification.
Recognize tradable patterns (e.g., breakouts, reversals) using a Convolutional Neural Network (CNN) to inform trading signals.
Generate trading signals (Buy/Sell/Hold) using NinjaTrader API data and Decision Trees.
Allow custom stock/instrument selection, train/test/validation splits, and sampling rates.
Provide backtesting with configurable strategies (e.g., ATR-based stop-loss, Fibonacci profit-taking) and metrics (e.g., Sharpe Ratio, Sortino Ratio).
Mitigate data, model, execution, backtesting, and system failures for robust real-world performance.
Input Data
Historical Dataset: User-selected stock/instrument (e.g., NQ, AAPL, BTC/USD) with 1-minute candles over 10 years.
Columns: Time, Open, High, Low, Close, Volume, EMA(9), EMA(21), EMA(220).
Sample: 2012-12-30,17:21:00,4323.5,4323.75,4323.25,4323.5,19,4323.66503824493,4322.29011793893,4318.87143055653.
Live Data: Real-time 1-minute candle data from NinjaTrader API (mocked as CSV for development).
System Requirements
Instrument Flexibility: Support any stock/instrument via CSV.
Custom Splits/Sampling: User-defined splits (e.g., 70/15/15) and sampling (e.g., 1-min to 5-min).
Models: Transformer for price prediction, CNN for pattern recognition.
Entropy: Shannon entropy with validation for signal confidence.
Backtesting: Configurable strategies with realistic simulations.
Mitigations: Address data quality, overfitting, latency, biases, and drift.
Modularity: Separate data, modeling, signaling, and backtesting.
Scalability: Docker-ready with Kafka stubs.
System Architecture
Modules:
data_service.py: Preprocesses data with validation.
model_service.py: Trains Transformer and CNN with regularization.
signal_service.py: Generates signals with entropy checks.
backtest_service.py: Simulates trades with realism.
main.py: Orchestrates flow with FastAPI and monitoring.
Tech Stack: Python, TensorFlow, Scikit-learn, Pandas, NumPy, FastAPI, Docker.
Detailed Prompt for Claude 3.5 Sonnet
Instructions for Claude:
Create a complete Python codebase for the "Smart Market Analyzer" using TensorFlow for Transformer (price prediction) and CNN (pattern recognition), Scikit-learn for Decision Trees, and FastAPI for endpoints. Mock NinjaTrader API with CSV reads. Integrate mitigations for data quality, overfitting, latency, backtesting biases, and drift as specified. Use the NQ sample row to generate mock data (100 rows). Output modular, commented, runnable code with a Dockerfile and requirements.txt.
Step 1: Data Preprocessing (data_service.py)  
Input: CSV path, config (split, sample_rate).
Tasks:
Parse Time as datetime index.
Validation: Check for missing values (fill with linear interpolation), outliers (e.g., High < Low, cap at 3σ), and liquidity (filter Volume < 100 trades/min).
Fallback: If CSV fails, attempt Yahoo Finance fetch (mocked as secondary CSV).
Normalize features to 0-1.
Splits: 70/15/15 chronological split; use K-means (3 clusters: bull, bear, sideways) to ensure regime coverage.
Sampling: Resample to 5-min, 15-min, 1-hour if specified (OHLCV mean); train on multi-resolution (1-min + resampled).
Output: X_train, y_train, etc., per timeframe.
Mitigations:
Poor Data Quality: Validation and fallback.
Sampling Bias: Multi-resolution and regime clustering.
Step 2: Model Development (model_service.py)  
Transformer for Price Prediction:
Input: [batch_size, window_size, 9] (e.g., [32, 60, 9]).
Architecture:
Multi-head self-attention (4 heads, d_model=64).
LayerNorm, Feedforward (64 units, ReLU), Dropout (0.2).
Dense (10 bins, softmax).
Entropy: 
H = -\sum p_i \log_2(p_i)
, Confidence: 
1 - H/\log_2(10)
; threshold at 0.75 
H_{\text{max}}
.
Training: Cross-entropy loss, Adam, early stopping (patience=10), time-series cross-validation (5 folds).
Regularization: L2 (0.01), dropout.
CNN for Pattern Recognition:
Input: [batch_size, window_size, 9, 1] (e.g., [32, 60, 9, 1]).
Architecture:
Conv2D (32 filters, 3×3), MaxPooling2D (2×2).
Conv2D (64 filters, 3×3), Flatten, Dense (128, ReLU), Dropout (0.2).
Dense (3 classes: Positive, Negative, Neutral).
Training: Categorical cross-entropy, Adam, early stopping.
Config:
json
{
  "instrument": "NQ",
  "price_model": "Transformer",
  "pattern_model": "CNN",
  "timeframe": "1-minute",
  "window_size": 60,
  "bins": 10,
  "num_heads": 4,
  "d_model": 64,
  "split": {"train": 0.7, "val": 0.15, "test": 0.15},
  "sample_rate": "1-minute",
  "entropy_threshold": 0.75
}
Output: Price prediction, pattern label, entropy, confidence.
Mitigations:
Overfitting: Dropout, L2, early stopping, cross-validation.
Entropy Misinterpretation: Thresholding, add variance check.
Step 3: Signal Generation (signal_service.py)  
Input: Transformer predictions, CNN patterns, latest candle.
Decision Tree:
Features: Predicted Close, Close, Volume, EMAs, entropy, pattern label.
Rules: Buy if prediction > Close + 0.1%, pattern = Positive, entropy < 0.75; Sell if opposite; Hold else.
Training: Fit on historical 1% moves; calibrate confidence with Platt scaling.
Output: Signal, confidence.
Mitigations:
Entropy Misinterpretation: Threshold and calibration.
Insufficient Risk: Volatility filter (pause if ATR > 2× avg).
Step 4: Backtesting Service (backtest_service.py)  
Strategy:
Config:
json
{
  "strategy": "Custom",
  "stop_loss": {"type": "ATR", "multiplier": 2, "period": 14},
  "profit_take": {"type": "Fibonacci", "level": 0.618},
  "entry": {"condition": "signal == 'Buy'"},
  "exit": {"condition": "signal == 'Sell'"},
  "slippage": 0.001,
  "max_position": 0.02,
  "max_daily_loss": 0.05
}
Options: ATR-based, Fixed, Trailing stop-loss; Fibonacci, Fixed, ATR profit-taking.
Realism: Add 0.1% slippage, $2/trade cost, latency (100ms mock delay).
Metrics: Sharpe, Sortino, Max Drawdown, Profit.
Process: Simulate on test set with strict past-data EMAs; walk-forward optimization (6-month windows); stress test with doubled volatility.
Mitigations:
Look-Ahead Bias: Past-only EMAs, out-of-sample test set.
Over-Optimization: Parameter ranges (e.g., ATR 1-3), walk-forward.
Slippage: Modeled in simulation, limit-order logic.
Risk: Position (2%) and loss (5%) caps.
Step 5: Main Application (main.py)  
FastAPI Endpoints:
/data: Validates config (e.g., train > 50%), processes data.
/train: Trains models, monitors MSE drift (retrain if >20% rise).
/predict: Batch-processes 100 candles, outputs predictions.
/signal: Generates signals with buffering (5 candles).
/backtest: Runs backtest with realism flags.
Monitoring: Log latency, entropy, profits (Prometheus stub).
Flow: Load → Train → Predict → Signal → Backtest.
Mitigations:
Latency: Batch processing, buffering.
Drift: MSE monitoring, retrain trigger.
Misconfiguration: Default config (70/15/15, ATR=2), validation checks.
Scalability: Batching, Docker load balancing.
Step 6: Deployment  
Dockerfile: Python 3.9, TensorFlow, FastAPI.
Mock NinjaTrader: CSV row reads with 100ms delay.
Optional: Kafka stub for live data.
Example Output
Instrument: NQ
Timeframe: 1-minute
  Prediction: 4324.0, Pattern: Positive, Entropy: 0.6, Confidence: 0.90
  Signal: Buy, Confidence: 0.92
Backtest (ATR Stop, Fib 0.618):
  Sharpe: 1.9, Sortino: 2.4, Profit: $5500
File Structure
smart_market_analyzer/
├── data_service.py
├── model_service.py
├── signal_service.py
├── backtest_service.py
├── main.py
├── Dockerfile
├── requirements.txt
├── config.json
├── sample_data.csv
Requirements
tensorflow==2.15.0
scikit-learn==1.5.0
fastapi==0.112.0
pandas==2.2.2
numpy==1.26.4
uvicorn==0.30.0
Why This Works
Mitigations Integrated: Validation, regularization, realism, and monitoring address all major failure modes.
Claude-Friendly: Detailed specs ensure accurate, runnable code.
End-to-End: From data to backtest, it’s a complete system.
Feed this into Claude 3.5 Sonnet, and you’ll get a robust codebase. Test with NQ data, tweak configs, and scale up. Let me know if you need a sample output or further adjustments!
